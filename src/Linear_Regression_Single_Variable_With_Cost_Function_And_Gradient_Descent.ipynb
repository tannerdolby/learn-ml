{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c822a92-857f-48fe-8f44-ea3e6ff48838",
   "metadata": {},
   "source": [
    "# Linear Regression Using One Variable With Cost Function and Gradient Descent\n",
    "\n",
    "Building off the previous notebook, I will implement a cost function and gradient descent to improve the linear regression model. Using the same training set of house sales from Redfin.\n",
    "\n",
    "Goals:\n",
    "- Optimize parameters `w` and `b` for the linear regression model programmatically using gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d7fa7f-469c-462e-806a-fa87573623ee",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "Predict the price of a house based on its size in sqft. The training set will include some recent house sales from [Redfin](https://www.realtor.com/realestateandhomes-search/Roseville_CA/show-recently-sold). Where the input features are house size (1000 sqft) and output targets are the closing sale price (1000s of dollars). For example, the first training example in the table below is a house size of 1464 sqft which sold for $215,000.\n",
    "\n",
    "| Size (1000 sqft) | Price (1000s of dollars) |\n",
    "|------------------|--------------------------|\n",
    "| 1.464            | 215.0                    |\n",
    "| 2.39             | 635.0                    |\n",
    "| 1.296            | 453.99                   |\n",
    "| 2.838            | 1100.0                   |\n",
    "| 2.37             | 673.0                    |\n",
    "| 2.362            | 775.0                    |\n",
    "| 1.343            | 559.9                    |\n",
    "| 1.966            | 651.0                    |\n",
    "| 1.974            | 565.0                    |\n",
    "| 2.08             | 765.873                  |\n",
    "| 0.974            | 452.5                    |\n",
    "| 1.334            | 480.0                    |\n",
    "| 1.822            | 730.0                    |\n",
    "| 2.853            | 1206.5                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4328e8-0cd2-4a7b-892a-fed2356e0480",
   "metadata": {},
   "source": [
    "## Linear Regression Model, Cost Function, and Gradient Descent\n",
    "Linear Regression Model:\n",
    "$$f_{w,b}(x)=wx+b\\tag{1}$$\n",
    "\n",
    "Cost Function:\n",
    "$$J(w,b)={1\\over2m}\\sum_{i=1}^m(f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$\n",
    "\n",
    "Gradient Descent Algorithm:\n",
    "Repeat until convergence {\n",
    "$$w = w - \\alpha * {\\partial\\over\\partial{w}}J(w,b)\\tag{3}$$\n",
    "$$b = b - \\alpha * {\\partial\\over\\partial{b}}J(w,b)\\tag{4}$$\n",
    "}\n",
    "\n",
    "### Terms\n",
    "Linear Regression Model parameters: $$w, b\\tag{5}$$\n",
    "Learning rate - positive value between [0, 1]: $$\\alpha\\tag{6}$$\n",
    "Partial derivative with respect to w:$${\\partial\\over\\partial{w}}\\tag{7}$$\n",
    "Partial derivative with respect to b:$${\\partial\\over\\partial{b}}\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c682509-a4d3-413a-99a4-b0dfe0d9738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('../deeplearning.mplstyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3f552f05-1672-433d-bb44-7d2c0dd379c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set\n",
    "x_train = np.array([1.464, 2.39, 1.296, 2.838, 2.370, 2.362, 1.343, 1.966, 1.974, 2.080, .974, 1.334, 1.822, 2.853])\n",
    "y_train = np.array([215, 635, 453.990, 1100, 673, 775, 559.90, 651, 565, 765.873, 452.5, 480, 730, 1206.50])\n",
    "m = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e34ebf11-eb6f-4288-83ca-a724f633686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_output(x, w, b):\n",
    "    \"\"\"\n",
    "    Compute linear regression model prediction\n",
    "    \"\"\"\n",
    "    m = x.shape[0]\n",
    "    f_wb = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        f_wb[i] = w * x[i] + b\n",
    "    return f_wb\n",
    "\n",
    "def compute_cost(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute cost function J(w,b)\n",
    "    \"\"\"\n",
    "    m = x.shape[0]\n",
    "    total_cost = 0\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost += (f_wb - y[i]) ** 2\n",
    "    total_cost = (1/2*m) * cost\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "badb9bda-1e8c-4bf2-b422-80c458ed828c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7601283.131603\n"
     ]
    }
   ],
   "source": [
    "w = 200\n",
    "b = 50\n",
    "\n",
    "cost = compute_cost(x_train, y_train, w, b)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4486d78-c8d6-4f8a-b7d0-cdbbf447db8b",
   "metadata": {},
   "source": [
    "TODO: Compute gradient, perform gradient descent, and plot things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31890e1-fc00-4d5b-9687-bfc420d5e0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
