{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c822a92-857f-48fe-8f44-ea3e6ff48838",
   "metadata": {},
   "source": [
    "# Linear Regression Using One Variable With Cost Function and Gradient Descent\n",
    "\n",
    "Building off the previous notebook, I will implement a cost function and gradient descent to improve the linear regression model. Using the same training set of house sales from Redfin.\n",
    "\n",
    "Goals:\n",
    "- Optimize parameters `w` and `b` for the linear regression model programmatically using gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d93b7bd-4a74-43bc-997b-4b2ccf6bc863",
   "metadata": {},
   "source": [
    "## Tools\n",
    "In this lab we will make use of:\n",
    "\n",
    "- NumPy, a popular library for scientific computing\n",
    "- Matplotlib, a popular library for plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b159ee74-f661-442d-bac7-6e6966e26a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.regression_helpers import plt_intuition, plt_stationary\n",
    "plt.style.use('../deeplearning.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d7fa7f-469c-462e-806a-fa87573623ee",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "Predict the price of a house based on its size in sqft. The training set will include some recent house sales from [Redfin](https://www.realtor.com/realestateandhomes-search/Roseville_CA/show-recently-sold). Where the input features are house size (1000 sqft) and output targets are the closing sale price (1000s of dollars). For example, the first training example in the table below is a house size of 1464 sqft which sold for $215,000.\n",
    "\n",
    "| Size (1000 sqft) | Price (1000s of dollars) |\n",
    "|------------------|--------------------------|\n",
    "| 1.464            | 215.0                    |\n",
    "| 2.39             | 635.0                    |\n",
    "| 1.296            | 453.99                   |\n",
    "| 2.838            | 1100.0                   |\n",
    "| 2.37             | 673.0                    |\n",
    "| 2.362            | 775.0                    |\n",
    "| 1.343            | 559.9                    |\n",
    "| 1.966            | 651.0                    |\n",
    "| 1.974            | 565.0                    |\n",
    "| 2.08             | 765.873                  |\n",
    "| 0.974            | 452.5                    |\n",
    "| 1.334            | 480.0                    |\n",
    "| 1.822            | 730.0                    |\n",
    "| 2.853            | 1206.5                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4328e8-0cd2-4a7b-892a-fed2356e0480",
   "metadata": {},
   "source": [
    "## Linear Regression Model, Cost Function, and Gradient Descent\n",
    "Linear Regression Model:\n",
    "$$f_{w,b}(x)=wx+b\\tag{1}$$\n",
    "\n",
    "Cost Function:\n",
    "$$J(w,b)={1\\over2m}\\sum_{i=0}^{m-1}(f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$\n",
    "\n",
    "Gradient Descent Algorithm:\n",
    "Repeat until convergence {\n",
    "$$w = w - \\alpha * {\\partial\\over\\partial{w}}J(w,b)\\tag{3}$$\n",
    "$$b = b - \\alpha * {\\partial\\over\\partial{b}}J(w,b)\\tag{4}$$\n",
    "}\n",
    "\n",
    "where the paramters `w` and `b` are updated simultaneously. The gradient is defined as:\n",
    "\n",
    "$${\\partial\\over\\partial{w}}J(w,b)={1\\over{m}}\\sum_{i=0}^{m-1}(f_{w,b}(x^{(i)}) - y^{(i)})*x^{(i)}\\tag{5}$$\n",
    "\n",
    "$${\\partial\\over\\partial{w}}J(w,b)={1\\over{m}}\\sum_{i=0}^{m-1}(f_{w,b}(x^{(i)}) - y^{(i)}\\tag{6})$$\n",
    "\n",
    "Calculating the parameters w and b simultaneously means calculating the partial derivative of all the parameters before updating the parameters with the new values.\n",
    "### Terms\n",
    "Linear Regression Model parameters: $$w, b\\tag{7}$$\n",
    "Learning rate - positive value between [0, 1]: $$\\alpha\\tag{8}$$\n",
    "Partial derivative with respect to w:$${\\partial\\over\\partial{w}}\\tag{9}$$\n",
    "Partial derivative with respect to b:$${\\partial\\over\\partial{b}}\\tag{10}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f552f05-1672-433d-bb44-7d2c0dd379c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set\n",
    "x_train = np.array([1.464, 2.39, 1.296, 2.838, 2.370, 2.362, 1.343, 1.966, 1.974, 2.080, .974, 1.334, 1.822, 2.853])\n",
    "y_train = np.array([215, 635, 453.990, 1100, 673, 775, 559.90, 651, 565, 765.873, 452.5, 480, 730, 1206.50])\n",
    "m = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66242dd-1ab0-4216-b13a-8c7a737c9692",
   "metadata": {},
   "source": [
    "## Implement Gradient Descent\n",
    "Implementing the gradient descent algorithm for one feature. We will need the following three functions:\n",
    "- `compute_cost`: implementing equation(2)\n",
    "- `compute_gradient`: implementing equation (5) and (6)\n",
    "- `gradient_descent`: utilizing compute_gradient and compute_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32d3d1-e4cb-4d30-9b15-5328c8146f60",
   "metadata": {},
   "source": [
    "### compute_cost\n",
    "`compute_cost` implements equation (2) and returns the total cost from the cost function J(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52e5e1fe-22b0-4b0a-8bd9-d35ffde75224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute cost function J(w,b)\n",
    "    Args:\n",
    "      x (ndarray): Shape (m,) Input to the model (size of houses)\n",
    "      y (ndarray): Shape (m,) Label (price of houses)\n",
    "      w,b (scalar):  Model parameters  \n",
    "    Returns\n",
    "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
    "               to fit the data points in x and y\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m = x.shape[0]\n",
    "\n",
    "    # total error cost\n",
    "    total_cost = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        # model prediction\n",
    "        f_wb = w * x[i] + b\n",
    "        # error cost\n",
    "        cost = (f_wb - y[i]) ** 2\n",
    "        # summation of error costs\n",
    "        total_cost += cost\n",
    "\n",
    "    total_cost = total_cost / (2*m)\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0081aa-403e-4a10-8c15-2375f97428d4",
   "metadata": {},
   "source": [
    "### compute_gradient\n",
    "`compute_gradient` implements equation (5) and (6) and returns the partial derivatives of the parameters: ${\\partial\\over\\partial{w}}J(w,b)$, ${\\partial\\over\\partial{b}}J(w,b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2578db4b-3bd9-4ca8-b7dd-100592684bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (m,) Input to the model (Size of house) \n",
    "      y (ndarray): Shape (m,) Label (Actual prices for the houses)\n",
    "      w, b (scalar): Parameters of the model  \n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m = x.shape[0]\n",
    "\n",
    "    # partial derivatives\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        # model prediction\n",
    "        f_wb = w * x[i] + b\n",
    "        # calculate partial derivative for ith training example\n",
    "        dj_dw_i = (f_wb - y[i]) * x[i]\n",
    "        dj_db_i = (f_wb - y[i])\n",
    "        # summation of derivatives\n",
    "        dj_dw += dj_dw_i\n",
    "        dj_db += dj_db_i\n",
    "\n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "\n",
    "    return dj_dw, dj_db\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acd943e-be24-45bb-9c8f-11a5beb07904",
   "metadata": {},
   "source": [
    "### gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5c2513c-bea0-4fbd-9ccb-72d0405bb280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "badb9bda-1e8c-4bf2-b422-80c458ed828c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for w=200, b=50: 38782.05679389286\n",
      "Gradient at initial w,b: -1397.25272 -661.6259285714285\n"
     ]
    }
   ],
   "source": [
    "w = 200\n",
    "b = 50\n",
    "\n",
    "cost = compute_cost(x_train, y_train, w, b)\n",
    "print(\"Cost for w=200, b=50:\", cost)\n",
    "\n",
    "# Calculate gradients with parameters initialized to 0\n",
    "w = 0\n",
    "b = 0\n",
    "tmp_dj_dw, tmp_dj_db = compute_gradient(x_train, y_train, w, b)\n",
    "print(\"Gradient at initial w,b:\", tmp_dj_dw, tmp_dj_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4486d78-c8d6-4f8a-b7d0-cdbbf447db8b",
   "metadata": {},
   "source": [
    "TODO: Perform batch gradient descent, and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e13f82-7e97-4fe7-a694-528c701c8b93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
